---
title: "chess_win_no_draws"
author: "A.Oh"
date: "2024-04-23"
output: html_document
---

Installing libraries:
```{r}
library(readr)
library(caret)
library(tidymodels)
library(tidyverse)
library(randomForest)
library(xgboost)
library(keras)
library(vip)
library(pROC)
library(dplyr)
library(MLmetrics)
library(ggplot2)
library(archive)
library(reticulate)

# replace with personal python venv path (make sure the python version is compatible with computer OS (x86 vs arm64))
use_python("/opt/anaconda3/envs/anaconda/bin/python")
py_install("pandas")
```

Loading csv from data.tar.gz file:
```{r}
chess_games_data <- read.csv(archive_extract("data.tar.gz", files="club_games_data.csv"))
head(chess_games_data)
```

Messing data up on purpose:
```{r}
set.seed(1)
# No. of rows to set as NA (0.0001% of your data)
num_rows_to_na <- ceiling(nrow(chess_games_data) * 0.0001)
# Columns to set as NA
columns_to_na <- c("white_rating", "black_rating", "rated", "time_class")

# Introduce NA values randomly across specified columns
for (col in columns_to_na) {
  print(col)
  # Randomly select rows for each column
  rows_to_na <- sample(1:nrow(chess_games_data), num_rows_to_na, replace = FALSE)
  # Set selected rows to NA for the column
  chess_games_data[rows_to_na, col] <- NA
}

write.csv(chess_games_data, "chess_games_data.csv", row.names = FALSE)
```

Data cleaning/pre-processing:

(Cleaning up the data)
1. Dropping the "black_result" column and setting the "white_result" column to either "win" or "loss".
```{r}
chess_games_data <- chess_games_data[, !(names(chess_games_data) %in% c("black_result"))]

chess_games_data <- chess_games_data %>%
  mutate(white_result = case_when(
    white_result %in% c("win", "threecheck", "kingofthehill") ~ "win",
    white_result %in% c("checkmated", "timeout", "resigned", "abandoned") ~ "loss",
  ))
```

2. Dropping "draw" columns
```{r}
chess_games_data <- chess_games_data[chess_games_data$white_result %in% c("win", "loss"), ]
unique(chess_games_data$white_result)
```


3. Splitting the "pgn" column/variable to extract data

```{python}
import pandas as pd

chess_games_data = r.chess_games_data

# Assuming 'chess_games_data' is already loaded with 'pgn' being one of its columns
pgn_column = chess_games_data['pgn']

# Extract specified metadata
feature_names = ['Event', 'Site']
feature_positions = [0, 1]

# Extract specified features from the PGN data
for feature_name, position in zip(feature_names, feature_positions):
    feature_col = []
    for row in pgn_column:
        text = row.split('\n')
        # Extracting the key-value pair, then stripping characters from the value
        key_value_pair = text[position].split(' ', 1)
        if len(key_value_pair) > 1:
            # Removing the first and last characters which should be the quote marks
            clean_value = key_value_pair[1].strip()[1:-1]
        else:
            clean_value = 'Unknown'  # Fallback if there's an issue with the format
        feature_col.append(clean_value)
    chess_games_data[feature_name] = feature_col

def extract_move(pgn):
    if(pgn.find('{[') == -1):
        original_list = pgn.split("\n")[-2].split()
        toberemoved_list = pgn.split("\n")[-2].split()[::3]
        new_list = [x for x in original_list if x not in toberemoved_list]
        return new_list
    else:
        return pgn.split("\n")[-2].split()[1::4]
chess_games_data['Moves'] = chess_games_data['pgn'].apply(extract_move)

r.extract = chess_games_data

# Now, extracting the first move properly
first_move_col = []
for row in pgn_column:
    # Splitting each PGN into sections, then getting the moves section
    sections = row.split(']')
    if len(sections) > 1:
        # Get the section after all metadata, clean up new lines and leading/trailing spaces
        moves_section = sections[-1].strip()
        # Split by spaces to isolate individual moves or move numbers
        moves = moves_section.split()
        # Assuming standard notation, the first actual move should follow the "1."
        if len(moves) > 1 and moves[0] == "1.":
            first_move = moves[1]  # First move typically follows "1."
        else:
            first_move = 'No move found'
    else:
        first_move = 'No move found'
    first_move_col.append(first_move)

chess_games_data['FirstMove'] = first_move_col

# extracting first move differently
first_move = []
for row in chess_games_data['Moves']:
  if len(row) == 1:
    first_move.append(row)
  elif len(row) > 1:
    first_move.append(row[0])
  else:
    first_move.append("No move found")
    
chess_games_data['FirstMove1'] = first_move

# First, verify the existing DataFrame structure by checking the first few rows
print("Before modification:")
print(chess_games_data[['Event', 'Site', 'FirstMove']].head())

before_mod = chess_games_data

# Drop columns safely
columns_to_drop = ['fen', 'Site', 'pgn', 'white_id', 'black_id', 'Eco', 'EcoName']
# Ensure all columns to drop exist in the DataFrame to avoid KeyError
existing_columns = [col for col in columns_to_drop if col in chess_games_data.columns]
chess_games_data = chess_games_data.drop(columns=existing_columns)

# Verify the DataFrame after dropping columns
print("After dropping columns:")
print(chess_games_data.head())
print(chess_games_data.columns)

r.chess_games_data = chess_games_data
r.before_mod = before_mod
```

Check that data has been transferred from Python to R
```{r}
colnames(chess_games_data)
```

Write changed data into a .csv file for saving purposes:
```{r}
write.csv(chess_games_data, "modified_games_data.csv", row.names = FALSE)
```

EDA
```{r}
original_chess_games_data  <- read.csv("club_games_data.csv")

ggplot(data = original_chess_games_data ) + geom_bar(aes(x = white_result),fill = "blue") + 
  labs(title = "Frequency of different outcomes for white players",
       x = "Outcomes", y = "Count") + theme_bw() +
       theme_minimal() +
       theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(data = original_chess_games_data ) + geom_bar(aes(x = black_result),fill = "blue") + 
  labs(title = "Frequency of different outcomes for black players",
       x = "Outcomes", y = "Count") + theme_bw() +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Frequency of different outcomes for white players and Frequency of different outcomes for black players:

# Both black players and white players exhibit a similar distribution of the frequency of different categorical outcomes.

# The win outcome is the most frequent, followed by resignation, timeout, and checkmated respectively. This indicates to us the common ways the games can conclude apart from the standard ways.

# Abandoned, repetition, timevsinsufficnet, and insufficient are less likely outcomes, respectively in descending order.

# threecheck and kingofthehill conditions are relatively rare.

# See the first 6 rows
head(original_chess_games_data )
## The count of missing values in the dataset
sum(is.na(original_chess_games_data ))
## The count of missing values by column
sapply(original_chess_games_data , function(col) sum(is.na(col)))
# Find the structure
str(original_chess_games_data )
# Print unique values in the white_result column
unique(original_chess_games_data $white_result)
# Summary
summary(original_chess_games_data )

library(e1071)

numeric_columns <- sapply(original_chess_games_data , is.numeric)

skewness_values <- sapply(original_chess_games_data [, numeric_columns], skewness, na.rm = TRUE)

# Pct change

(0.04805706-0.04671716)/0.04671716

# Plotting the distribution of numerical features

ggplot(gather(original_chess_games_data [numeric_columns]), aes(value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~ key, scales = 'free_x') +
  theme_minimal() +
  labs(title = 'Distribution of Numerical Features', x = NULL, y = 'Frequency')

# Distribution of Numerical Features

# Black rating ELO values exhibit 2.9% higher skew based on the percent change formula than do white rating ELO values.

ggplot(original_chess_games_data , aes(x = white_rating, fill = white_result)) + 
  geom_density(alpha = 0.2) +
  labs(title = "Density Plot of White Ratings by Game Outcome",
       x = "White Rating", y = "Density") +
  scale_fill_manual(values = c("win" = "blue", "checkmated" = "red"))

# For black

ggplot(original_chess_games_data , aes(x = black_rating, fill = black_result)) + 
  geom_density(alpha = 0.2) +
  labs(title = "Density Plot of Black Ratings by Game Outcome",
       x = "White Rating", y = "Density") +
  scale_fill_manual(values = c("win" = "blue", "checkmated" = "red"))

# Density Plot of White Ratings by Game Outcome

# Density Plot of Black Ratings by Game Outcome

# The win curve is broader than the checkmated curve, and captures higher-rated players as one would expect.
# Higher rated players are more likely to be checkmated than lower-rated players, and the checkmated curve tends to have less extreme values than the win curve does.

# For white

ggplot(original_chess_games_data , aes(x = white_result, y = white_rating, fill = white_result)) +
  geom_boxplot() +
  labs(title = "Boxplot of White Ratings by Game Outcome",
       x = "Outcome", y = "Rating") +
  scale_fill_manual(values = c("win" = "blue", "checkmated" = "red")) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# For black

ggplot(original_chess_games_data , aes(x = black_result, y = black_rating, fill = black_result)) +
  geom_boxplot() +
  labs(title = "Boxplot of Black Ratings by Game Outcome",
       x = "Outcome", y = "Rating") +
  scale_fill_manual(values = c("win" = "blue", "checkmated" = "red")) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Boxplot of White Ratings by Game Outcome:

# Boxplot of Black Ratings by Game Outcome:

# For both black and white players, for the wins, the median ELO is slightly higher than for checkmated outcomes, as well as the interquartile range, which is transposed a bit higher.

# Most notably, there are far more extremely-high, outlier values.

numeric_data <- select(original_chess_games_data , white_rating, black_rating)

pca_results <- prcomp(numeric_data, center = TRUE, scale = TRUE)

library("dplyr")

biplot(pca_results, scale = 0)

# PCA (Principal Component Analysis):

# Biplot:

# The black and white rating vectors both point leftward, indicating that they correlate highly, which makes sense because in games similar ELOs are paired.

# The arrows are relatively long, meaning both the white rating and the white rating are important variables for explaining observational variability of the other features, albeit the black rating very slightly moreso.

# Ratings are an indispensable factor in predicting outcomes. High ratings indicate that players tend to have better understanding and mastery of the game.

# We witness a broad distribution, implying a diverse set of games which involves players of different skill levels.
```


4. Data Splitting
```{r}
set.seed(1)      

index <- createDataPartition(chess_games_data$`white_result`, p = 0.70, list = FALSE)
games_train <- chess_games_data[index, ]
games_test <- chess_games_data[-index, ]
```


5. Combine the datasets into one data frame with an additional column indicating the dataset source
```{r}
combined_df <- rbind(data.frame(data = 'white_result', dataset = 'Full', value = chess_games_data$`white_result`),
                     data.frame(data = 'white_result', dataset = 'Train', value = games_train$`white_result`),
                     data.frame(data = 'white_result', dataset = 'Test', value = games_test$`white_result`))
combined_df <- combined_df %>%
  group_by(dataset, value) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count)) %>%
  ungroup()

ggplot(combined_df, aes(x = value, y = percentage, fill = dataset)) +
  geom_bar(position = "dodge", stat = "identity") +
  theme_minimal() +
  labs(title = "Distribution of Target Feature 'white_result' across Datasets",
       x = "Target Feature 'white_result' Value",
       y = "Percentage") +
  scale_fill_manual(values = c("Full" = "tomato", "Train" = "blue", "Test" = "grey"))
```

6. Transforming data
```{r}
blueprint <- recipe(white_result ~ ., data = games_train) %>%
  step_string2factor(all_nominal_predictors()) %>%
  #step_log(all_numeric_predictors(), skip = TRUE) %>%
  step_nzv(all_predictors()) %>%
  step_impute_knn(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  step_other(all_nominal(), threshold = 0.01, other = "other") %>%
  step_dummy(all_nominal_predictors())

blueprint_prep <- prep(blueprint, training = games_train)


transformed_train <- bake(blueprint_prep, new_data = games_train)
#write.csv(transformed_train, "~/Documents/Chess Win Prediction/transformed_train.csv", row.names = FALSE)
#transformed_train <- read.csv("~/Documents/chess win prediction/transformed_train.csv")

transformed_test <- bake(blueprint_prep, new_data = games_test)
#write.csv(transformed_test, "~/Documents/Chess Win Prediction/transformed_test.csv", row.names = FALSE)
#transformed_test <- read.csv("~/Documents/chess win prediction/transformed_test.csv")

transformed_train$white_result <- factor(transformed_train$white_result, levels = c("win", "loss"))
transformed_test$white_result <- factor(transformed_test$white_result, levels = c("win", "loss"))
```

Modeling:
1. Random Forest
- feature engineering
```{r}
resample_1 <- trainControl(method = "cv",
                           number = 5,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)    # adds more metrics to the model

hyper_grid <- expand.grid(mtry = c(13, 15, 17),
                          splitrule = c("gini", "extratrees"),
                          min.node.size = c(5, 7, 9))

rf_fit <- train(white_result ~ .,
                data = transformed_train, 
                method = "ranger",
                verbose = FALSE,
                trControl = resample_1, 
                tuneGrid = hyper_grid,
                metric = "ROC")

saveRDS(rf_fit, "chess win prediction/rf_model.rds") # for convenience 
rf_fit <- readRDS("chess win prediction/rf_model.rds")

ggplot(rf_fit, metric = "Sens")   # Sensitivity
ggplot(rf_fit, metric = "Spec")   # Specificity
ggplot(rf_fit, metric = "ROC")    # AUC ROC
```

- Final Model
```{r}
fitControl_final <- trainControl(method = "none",
                                 classProbs = TRUE)

RF_final <- train(white_result ~., 
                  data = transformed_train,
                  method = "ranger",
                  trControl = fitControl_final,
                  metric = "ROC",
                  tuneGrid = data.frame(.mtry = 13,
                                        .min.node.size = 9,
                                        .splitrule = "gini"))
```

- Fitting final model to training and test sets:
    - Training set results
```{r}
RF_pred_train <- predict(RF_final, newdata = transformed_train)

RF_train_results <- confusionMatrix(transformed_train$white_result, RF_pred_train)

print(RF_train_results)

RF_Kappa <- RF_train_results$overall["Kappa"]
```

    - Testing set results
```{r}
RF_pred_test <- predict(RF_final, newdata = transformed_test)

RF_test_results <- confusionMatrix(transformed_test$white_result, RF_pred_test)

print(RF_test_results)
```

2. XG Boost
- feature engineering
```{r}
dtrain <- xgb.DMatrix(data = as.matrix(transformed_train[, -which(names(transformed_train) == "white_result")]),
                      label = as.numeric(transformed_train$white_result) - 1)

dtest <- xgb.DMatrix(data = as.matrix(transformed_test[, -which(names(transformed_test) == "white_result")]),
                     label = as.numeric(transformed_test$white_result) - 1)

hyper_grid <- expand.grid(
  eta = c(0.05, 0.1),
  max_depth = c(3, 6),
  min_child_weight = c(1, 2),
  subsample = c(0.5, 0.75),
  colsample_bytree = c(0.5, 0.75),
  gamma = c(0, 0.1),
  stringsAsFactors = FALSE  # To keep the grid as a dataframe of strings
)

# Initialize an empty dataframe to store results
results <- data.frame()

# Loop over each set of parameters in the grid
for(i in 1:nrow(hyper_grid)) {
  params <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric = "auc",
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i],
    gamma = hyper_grid$gamma[i]
  )
  
  # Perform cross-validation
  cv <- xgb.cv(
    params = params,
    data = dtrain,
    nrounds = 100,
    nfold = 5,
    early_stopping_rounds = 10,
    verbose = 0
  )
  
  # Record the best score and corresponding parameters
  best_score <- max(cv$evaluation_log$test_auc_mean)
  results <- rbind(results, cbind(hyper_grid[i, ], score = best_score))
}

# Review the results
results <- results[order(-results$score), ]  # Sort by score descending
print(results)

best_params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.1,
  max_depth = 6,
  min_child_weight = 2,
  subsample = 0.50,
  colsample_bytree = 0.75,
  gamma = 0.1
)
```

- Final model
```{r}
XGB_final <- xgb.train(
  params = best_params,
  data = dtrain,
  nrounds = 100  # Or use the best iteration number from your CV results
)
```

- Training set results
```{r}
XGB_pred_train <- predict(XGB_final, dtrain)
XGB_pred_train <- ifelse(XGB_pred_train > 0.5, 1, 0)  # Convert probabilities to binary class output
```

- Testing set
```{r}
# Prepare the testing data as DMatrix
dtest <- xgb.DMatrix(data = as.matrix(transformed_test[, -which(names(transformed_test) == "white_result")]))
# Predictions on the testing data
XGB_pred_test <- predict(XGB_final, dtest)
XGB_pred_test <- ifelse(XGB_pred_test > 0.5, 1, 0)  # Convert probabilities to binary class output

# Training data confusion matrix
confusionMatrix(factor(XGB_pred_train, levels = c(0, 1)),
                factor(as.numeric(transformed_train$white_result) - 1, levels = c(1, 0)))

XGB_Kappa <- XGB_train_results$overall["Kappa"]
```

- Confusion Matrix for testing data
```{r}
confusionMatrix(factor(XGB_pred_test, levels = c(0, 1)),
                factor(as.numeric(transformed_test$white_result) - 1, levels = c(1, 0)))
```

3. Artificial Neural Networks
- feature engineering
```{r}
resample <- trainControl(method = "cv",
                         
                         number = 5,
                         
                         classProbs = TRUE,
                         
                         summaryFunction = twoClassSummary)


hyper_grid <- expand.grid(layer1 = c(25, 45),
                          
                          layer2 = c(10, 30),
                          
                          layer3 = c(5, 15))

# Tuning hyperparameters

dnn_fit <- train(white_result ~ .,
                 
                 data = transformed_train, 
                 
                 method = "mlpML",
                 
                 trControl = resample, 
                 
                 tuneGrid = hyper_grid,
                 
                 metric = "ROC")

print(dnn_fit$results)

ggplot(dnn_fit, metric = "Sens")   # Sensitivity
ggplot(dnn_fit, metric = "Spec")   # Specificity
ggplot(dnn_fit, metric = "ROC")    # AUC ROC
```

- Final model
```{r}
fitControl_final <- trainControl(method = "none",
                                 classProbs = TRUE)

ANN_final <- train(white_result ~., 
                   data = transformed_train,
                   method = "mlpML",
                   trControl = fitControl_final,
                   metric = "ROC",
                   tuneGrid = expand.grid(layer1 = 45,
                                          layer2 = 30,
                                          layer3 = 5))
```

- Training set results
```{r}
ANN_pred_train <- predict(ANN_final, newdata = transformed_train)

ANN_train_results <- confusionMatrix(transformed_train$white_result, ANN_pred_train)

print(ANN_train_results)

ANN_Kappa <- ANN_train_results$overall["Kappa"]
```

- Testing set results
```{r}
ANN_pred_test <- predict(ANN_final, newdata = transformed_test)

ANN_test_results <- confusionMatrix(transformed_test$white_result, ANN_pred_test)

print(ANN_test_results)
```

4. Super Learner (SP)
- weights
```{r}
Weights <- c((RF_Kappa)^2/sum((RF_Kappa)^2 + (XGB_Kappa)^2 + (ANN_Kappa)^2),
             
             (XGB_Kappa)^2/sum((RF_Kappa)^2 + (XGB_Kappa)^2 + (ANN_Kappa)^2),
             
             (ANN_Kappa)^2/sum((RF_Kappa)^2 + (XGB_Kappa)^2) + (ANN_Kappa)^2)


super_learner <- function(M1, M2, M3, W) {
  
  weighted_average <- t(W*t(cbind(M1, M2, M3))) %>% apply(1, sum)
  
  final_prediction <- ifelse(weighted_average > 0.5, "win", "loss") %>% factor(levels = c("loss", "win"))
  
  return(final_prediction)
  
}
```

- Making predictions
```{r}
RF_prediction <- predict(RF_final, newdata = games_test, type = "prob")["win"]

SVM_prediction <- predict.train(SVM_final, newdata = games_test %>% select(-white_result), type = "prob")["win"]

XGB_prediction <- predict(XGB_final, newdata = games_test, type = "prob")["win"]

ANN_prediction <- predict(ANN_final, newdata = games_test, type = "prob")["win"]

SP_prediction <- super_learner(RF_prediction, XGB_prediction, ANN_prediction, Weights)

SP_prediction_factor <- factor(ifelse(SP_prediction > 0.5, "win", "loss"),
                               levels = c("loss", "win")) # fixes the weird order

SP_results <- confusionMatrix(transformed_test$white_result, SP_prediction)

SP_results
```

- Interpretable ML: Permutation Based Feature Importance
```{r}
# RF

vip(RF_final)

vip(RF_final, num_features = 20)

vip(RF_final, num_features = 20, geom = "point")


# XGB

vip(XGB_final)

vip(XGB_final, num_features = 20)

vip(XGB_final, num_features = 20, geom = "point")

# ANN

prob_yes <- function(object, newdata) {                        # wrapper function
  
  predict(object, newdata = newdata, type = "prob")[, "Yes"]
  
}


vip(ANN_final, method = "permute", train = transformed_train, target = "white_result",
    
    metric = "roc_auc", reference_class = "Yes", pred_wrapper = prob_yes)
